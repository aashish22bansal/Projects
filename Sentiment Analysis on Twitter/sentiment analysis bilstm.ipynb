{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('C:\\\\SPB_Data\\\\IIT Kanpur Applied Machine Learning and Data Science\\\\Kaggle Project\\\\Sentiment Analysis on Twitter\\\\train.txt'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing all the libraries\n",
    "\n",
    "import sys , os , re , csv , codecs , numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense,Input,LSTM,Embedding,Dropout,Activation,BatchNormalization\n",
    "from keras.layers import Bidirectional,GlobalMaxPool1D,GlobalAvgPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints , optimizers, layers\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the train data\n",
    "train = pd.read_csv('C:\\\\SPB_Data\\\\IIT Kanpur Applied Machine Learning and Data Science\\\\Kaggle Project\\\\Sentiment Analysis on Twitter\\\\train.txt')\n",
    "#loading the test data\n",
    "test = pd.read_csv('C:\\\\SPB_Data\\\\IIT Kanpur Applied Machine Learning and Data Science\\\\Kaggle Project\\\\Sentiment Analysis on Twitter\\\\test_samples.txt')\n",
    "#diplay first 5 rows of train\n",
    "train.head()\n",
    "#one hot encoding the labels\n",
    "df = pd.concat([train,pd.get_dummies(train['sentiment'])],axis=1)\n",
    "#df.head()\n",
    "train_data = df['tweet_text']\n",
    "#train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @jjuueellzz down in the Atlantic city, ventnor...\n",
       "1    Musical awareness: Great Big Beautiful Tomorro...\n",
       "2    On Radio786 100.4fm 7:10 Fri Oct 19 Labour ana...\n",
       "3    Kapan sih lo ngebuktiin,jan ngomong doang Susa...\n",
       "4    Excuse the connectivity of this live stream, f...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test['tweet_text']\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       ...,\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the array of labels in serial with their respective texts\n",
    "classes = ['neutral' , 'negative' , 'positive']\n",
    "y = df[classes].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id      False\n",
       "tweet_text    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for null values in train and test data\n",
    "train.isnull().any()\n",
    "test.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration  parameters\n",
    "LATENT_DIM_DECODER = 400\n",
    "BATCH_SIZE =128\n",
    "EPOCHS = 20\n",
    "LATENT_DIM = 400\n",
    "NUM_SAMPLES = 10000\n",
    "MAX_SEQUENCE_LEN = 1000\n",
    "MAX_NUM_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK python library for preprocessing\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#for tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#for stemming\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "#for removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "#importing regex library of python\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "#function for performing all preproccing steps at once\n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence=sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens]#  if not w in stopwords.words('english')]\n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "#make a dataframe of preprocessed text\n",
    "df['cleanText']=train_data.map(lambda s:preprocess(s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       jjuueellzz down in the atlantic city ventnor m...\n",
       "1       musical awareness great big beautiful tomorrow...\n",
       "2       on radio fm fri oct labour analyst shawn hatti...\n",
       "3       kapan sih lo ngebuktiin jan ngomong doang susa...\n",
       "4       excuse the connectivity of this live stream fr...\n",
       "5       show your love for your local field it might w...\n",
       "6       milton on bolton wanderers v leeds united sat ...\n",
       "7       firecore can you tell me when an update for th...\n",
       "8       heavensbasement the crown filthy mcnastys katy...\n",
       "9       uncover the eternal city return flights to rom...\n",
       "10      my cre blog oklahoma per square foot returns t...\n",
       "11      going to helsinki tomorrow or on the day after...\n",
       "12      trey burke has been suspended for the northern...\n",
       "13      w o w wednesday marni lands this lumberjack ve...\n",
       "14      activists in deir ezzor captured this image of...\n",
       "15      karaotr you will appreciate this sunday brunch...\n",
       "16      no school for me and nat tomorrow sucks for yo...\n",
       "17      join me wed for a live webcast on cost optimiz...\n",
       "18      nvsofficial saw you in glasgow loveeedd dropde...\n",
       "19      special thanks to everyone for coming out to t...\n",
       "20      fatimasule that was the revelation i mentioned...\n",
       "21      kim hyung jun football team the nd a match at ...\n",
       "22      the audio booth is ready to blow the roof off ...\n",
       "23      just seen jonasbrothers at radio city and now ...\n",
       "24      i may or may not be jumping around the lounge ...\n",
       "25      whos trying to go to buenos aires with me febr...\n",
       "26      trirob just bought tickets to see george grove...\n",
       "27      jeremyonmarz just be tryna hate on ma nd babyd...\n",
       "28      looks like im going to the uga vs ole miss gam...\n",
       "29      killa_ if you ain t doing nothing saturday sli...\n",
       "                              ...                        \n",
       "5368    the next baby boom is going to be in september...\n",
       "5369    seattle marathon coming through the arboretum ...\n",
       "5370    why would they have hot jam on a school night ...\n",
       "5371    i m going to celtic thunder at f m kirby cente...\n",
       "5372    thejasonmichael another thing not taking into ...\n",
       "5373    found it evanescence live rock im park in germ...\n",
       "5374    th annual polish festival is one for the bucke...\n",
       "5375    jonathan kozol event cancelled for friday even...\n",
       "5376    st_jamestheatre the ladykillers which i first ...\n",
       "5377    saraxoxo i d love that hmu sometime this week ...\n",
       "5378    symigoddess thanks i can t wait for my tasting...\n",
       "5379    i made the leftover halloween candy last until...\n",
       "5380    with veteran day sun i want you to thank all t...\n",
       "5381    beth tweddle greg rutherford and luke campbell...\n",
       "5382                  de la soul tomorrow night go get it\n",
       "5383    simplysolids i m hoping to make my last few xm...\n",
       "5384    new post jessica biel at the espy awards in lo...\n",
       "5385    on a friday night amp i m in bed with a cup of...\n",
       "5386    glinner s stage adaptation of the ladykillers ...\n",
       "5387    well if the world is going to end i may as wel...\n",
       "5388    just finished watching st episode of accused f...\n",
       "5389    i m getting the virus i had during thanksgivin...\n",
       "5390    some of our photos from the dallas_tnt premier...\n",
       "5391    wednesday at roskilde festival was the best ex...\n",
       "5392    danny grainger is out of the edinburgh derby o...\n",
       "5393    it s a wednesday girls night out as s band wil...\n",
       "5394    night college course sorted just have to enrol...\n",
       "5395    for the st time in years for your splendiferou...\n",
       "5396    nurses day may nursing the heart beat of the h...\n",
       "5397    we have minutes left until the nd episode of s...\n",
       "Name: clean_text, Length: 5398, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['clean_text']=test['tweet_text'].map(lambda s:preprocess(s))\n",
    "test_final = test['clean_text']\n",
    "test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#breaking the sentence into unique words/tokens\n",
    "#expecting max tokens to be 20k\n",
    "train_final = df['cleanText']\n",
    "max_feat=40000\n",
    "#tokenize sentence into list of words\n",
    "tokenizer = Tokenizer(num_words=max_feat)#setting up tokenizer\n",
    "#fiiting the tokenizer on out data\n",
    "tokenizer.fit_on_texts(list(train_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        gas by my house hit i um going to chapel hill ...\n",
       "1        theo walcott is still shit uc watch rafa and j...\n",
       "2        its not that i um a gsp fan uc i just hate nic...\n",
       "3        iranian general says israel us iron dome can u...\n",
       "4        tehran uc mon amour obama tried to establish t...\n",
       "5        i sat through this whole movie just for harry ...\n",
       "6        with j davlar th main rivals are team poland h...\n",
       "7        talking about act us sat us uc deciding where ...\n",
       "8        why is happy valentines day trending it us on ...\n",
       "9        they may have a superbowl in dallas uc but dal...\n",
       "10       im bringing the monster load of candy tomorrow...\n",
       "11       apple software uc retail chiefs out in overhau...\n",
       "12       oluoch victor_otti kunjand i just watched it s...\n",
       "13       one of my best th graders kory was excited aft...\n",
       "14       livewire nadal confirmed for mexican open in f...\n",
       "15       msshelahy i didnt want to just pop up but yep ...\n",
       "16       alyoup addictedhaley hmmmm november is an odd ...\n",
       "17       iran us delisting mko from global terrorists l...\n",
       "18       jackstirling serge is amazing like hes actuall...\n",
       "19       hatersgonnhate_ hamlikehussain ramythelite lar...\n",
       "20       good morning becky thursday is going to be fan...\n",
       "21       expect light moderate rains over e visayas ceb...\n",
       "22       one ticket left for the ers game tomorrow don ...\n",
       "23       afc away fans on saturday all this stuff about...\n",
       "24       my saturday night has consisted of me watching...\n",
       "25       why is it so hard to find the tvguidemagazine ...\n",
       "26       game of the nlcs and a rematch of the nfc cham...\n",
       "27       james hall live in indianapolis dec th christ ...\n",
       "28       trevorjavier the heat game may cost alot more ...\n",
       "29       never start working on your dreams and goals t...\n",
       "                               ...                        \n",
       "21435    don t miss how to bake a man with jessicainlan...\n",
       "21436    my_bird_tweets how you feeling about the colts...\n",
       "21437    sangam university bhilwara salutes nobel laure...\n",
       "21438    vinaykumargupta modi will come and go dynastie...\n",
       "21439    the fia ran the race the right way lauda niki ...\n",
       "21440    experts reveal the best social media apps for ...\n",
       "21441    woo hoo it s friday stephen colbert sings frid...\n",
       "21442    if you coming to my house saturday in the morn...\n",
       "21443    me amp nia was having the best convo about rel...\n",
       "21444    guess i m going to knott s scary farm on wedne...\n",
       "21445    johnwkyc green bay drafted rogers at about the...\n",
       "21446    joseleatleti dave_llb aaah st fixture list i s...\n",
       "21447    abc giant pandas may be cute but it takes a lo...\n",
       "21448    condole the passing of lee soo man s wife from...\n",
       "21449    alonso is still a ferrari driver today but he ...\n",
       "21450    heading to west wylong tomorrow for a couple o...\n",
       "21451    october airbridgecargo airlines abc has appoin...\n",
       "21452    yo annietudzin you excited for this packers ga...\n",
       "21453    so knott s tomorrow lmao ima have my bro drive...\n",
       "21454    niki lauda just confirmed to sky that alonso w...\n",
       "21455    imtommyhill brianlawless i notice niki lauda a...\n",
       "21456    deep condolance for lee soo man s wife may she...\n",
       "21457    just means two of atleti real or barca will be...\n",
       "21458    finishing up the fec pprwrk for rd quarter ton...\n",
       "21459    oct rock on dandiya nite at sangam university ...\n",
       "21460    the day after newark ill be able to say i met ...\n",
       "21461    fec hold farewell session for seven ministers ...\n",
       "21462    luca di montezemolo who s last day was monday ...\n",
       "21463    coffee is pretty much the answer to all questi...\n",
       "21464    niki lauda just confirmed to sky that alonso w...\n",
       "Name: cleanText, Length: 21465, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = Tokenizer(num_words=max_feat)#setting up tokenizer\n",
    "#fiiting the tokenizer on out data\n",
    "tokenizer2.fit_on_texts(list(test_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34302 unique input tokens.\n"
     ]
    }
   ],
   "source": [
    "#converting text into sequence of numbers to feed in neural network\n",
    "sequence_train = tokenizer.texts_to_sequences(train_final)\n",
    "sequence_test = tokenizer2.texts_to_sequences(test_final)\n",
    "# get the word to index mapping for input language\n",
    "word2idx_inputs = tokenizer.word_index\n",
    "print('Found %s unique input tokens.' % len(word2idx_inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/glove.6B.300d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-0e5c8fe8cfca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loading word vectors...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mword2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/kaggle/input/glove.6B.300d.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;31m# is just a space-separated text file in the format:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# word vec[0] vec[1] vec[2] ...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/glove.6B.300d.txt'"
     ]
    }
   ],
   "source": [
    "#LOADING PRETRAINED WORD VECTORS\n",
    "# store all the pre-trained word vectors\n",
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "with open('/kaggle/input/glove.6B.300d.txt', encoding=\"utf8\") as f:\n",
    "    # is just a space-separated text file in the format:\n",
    "    # word vec[0] vec[1] vec[2] ...\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "word2vec[word] = vec\n",
    "print('Found %s word vectors.' % len(word2vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n"
     ]
    }
   ],
   "source": [
    "#EMBEDDING MATRIX\n",
    "# prepare embedding matrix of words for embedding layer\n",
    "print('Filling pre-trained embeddings...')\n",
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx_inputs.items():\n",
    "    if(i < MAX_NUM_WORDS):\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074\n"
     ]
    }
   ],
   "source": [
    "max_len = [len(s) for s in sequence_train]\n",
    "print(max(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling all the sequences to a fixed length\n",
    "#dimension of input to the layer should be constant\n",
    "#scaling each comment sequence to a fixed length to 200\n",
    "#comments smaller than 200 will be padded with zeros to make their length as 200\n",
    "max_len=1000\n",
    "#pad the train and text sequence to be of fixed length (in keras input in lstm should be of fixed length sequnece)\n",
    "x_train=pad_sequences(sequence_train,maxlen=max_len)\n",
    "x_test=pad_sequences(sequence_test,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding layer\n",
    "embedding_layer = Embedding(\n",
    "  num_words,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  input_length=max_len,\n",
    "  trainable=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if we used right length of sequence padding for greater length of comments we have to take such length sequence padding that on deleting some information doesnt result in loss of important imformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_words = [len(words) for words in sequence_train]\n",
    "#distribution of sequence\n",
    "plt.hist(len_words, bins = np.arange(0,400,10))\n",
    "plt.show()\n",
    "# we can see that most of the comments have [0,50]  words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape=(max_len,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feeding the output of previous layer to the embedding layer that converts \n",
    "#the sequences into vector representation to detect relevance and context \n",
    "#of a particular word\n",
    "embed_layer =embedding_layer(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing the previous output as input to the BI_LSTM layer\n",
    "LSTM_layer = Bidirectional(LSTM(256, return_sequences=True, name='BI_lstm_layer'))(embed_layer)\n",
    "sec_LSTM_layer = Bidirectional(LSTM(256, return_sequences=True, name='BI2_lstm_layer'))(LSTM_layer)\n",
    "batchnorm = BatchNormalization()(sec_LSTM_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimension reduction using pooling layer\n",
    "red_dim_layer = GlobalAvgPool1D()(batchnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### adding dropout layer for better generalization\n",
    "#setting value as 0.1 , which means 10$ of nodes will be randomly disabled\n",
    "drop_layer = Dropout(0.55)(red_dim_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#densely connected layer\n",
    "dense1 = Dense(128,activation='elu')(drop_layer)\n",
    "batchnorm2 = BatchNormalization()(dense1)\n",
    "dense2 = Dense(128,activation='elu')(batchnorm2)\n",
    "batchnorm3 = BatchNormalization()(dense2)\n",
    "dense3 = Dense(128,activation='elu')(batchnorm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding another dropout layer\n",
    "drop_layer2 = Dropout(0.55)(dense3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the output dense layer with sigmoid activation to get result \n",
    "#3  classes as output\n",
    "output_dense = Dense(3,activation='softmax')(drop_layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting the inputs and outputs to create a model and compiling the model\n",
    "from keras.optimizers import Adagrad,Adam,RMSprop\n",
    "model = Model(inputs=input , outputs = output_dense)\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = RMSprop(lr=0.001),\n",
    "             metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model \n",
    "batch_size=64\n",
    "epochs = 30\n",
    "model.fit(x_train,y,batch_size=batch_size,epochs = epochs,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
